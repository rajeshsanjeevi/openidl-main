stages:
  - name: BUILD
    inputs:
      - type: git
        branch: stage
        dir_name: null
        service: hostedgitetl
    permission:
      execute: TOOLCHAIN_ADMINS
    properties:
      - name: IMAGE_NAME
        value: openidl-etl
        type: text
    jobs:
      - name: Fetch code
        type: builder
        artifact_dir: ""
        build_type: shell
        script: |
          #!/bin/bash
          # set -x

          # Git repo cloned at $WORKING_DIR, copy into $ARCHIVE_DIR
          mkdir -p $ARCHIVE_DIR
          cp -R -n ./ $ARCHIVE_DIR/ || true

          # Record git info
          echo "GIT_URL=${GIT_URL}" >> $ARCHIVE_DIR/build.properties
          echo "GIT_BRANCH=${GIT_BRANCH}" >> $ARCHIVE_DIR/build.properties
          echo "GIT_COMMIT=${GIT_COMMIT}" >> $ARCHIVE_DIR/build.properties
          echo "SOURCE_BUILD_NUMBER=${BUILD_NUMBER}" >> $ARCHIVE_DIR/build.properties
          cat $ARCHIVE_DIR/build.properties

          # check if doi is integrated in this toolchain
          if jq -e '.services[] | select(.service_id=="draservicebroker")' _toolchain.json; then
            # Record build information
            ibmcloud login --apikey ${IBM_CLOUD_API_KEY} --no-region
            ibmcloud doi publishbuildrecord --branch ${GIT_BRANCH} --repositoryurl ${GIT_URL} --commitid ${GIT_COMMIT} \
              --buildnumber ${BUILD_NUMBER} --logicalappname ${IMAGE_NAME} --status pass
          fi
      - name: Unit Tests
        type: tester
        script: |-
          #!/bin/bash
          # set -x
          if [ -f ./tests/run-tests.sh ]; then
            source ./tests/run-tests.sh
            RESULT=$?
            if [ ! -z "${FILE_LOCATION}"]; then
              if [ ${RESULT} -ne 0 ]; then STATUS=fail; else STATUS=pass; fi
                if jq -e '.services[] | select(.service_id=="draservicebroker")' _toolchain.json; then
                  ibmcloud login --apikey ${IBM_CLOUD_API_KEY} --no-region
                  ibmcloud doi publishtestrecord --type unittest --buildnumber ${BUILD_NUMBER} --filelocation ${FILE_LOCATION} \
                    --buildnumber ${BUILD_NUMBER} --logicalappname ${IMAGE_NAME} --status ${STATUS}
                fi
              exit $RESULT
            fi
          else
            echo "Test runner script not found: ./tests/run-tests.sh"
          fi
        test_type: simple
  - name: CONTAINERIZE
    inputs:
      - type: job
        stage: BUILD
        job: Fetch code
        dir_name: null
    triggers:
      - type: stage
    permission:
      execute: TOOLCHAIN_ADMINS
    properties:
      - name: buildprops
        value: build.properties
        type: file
      - name: DOCKER_ROOT
        value: .
        type: text
      - name: DOCKER_FILE
        value: Dockerfile
        type: text
      - name: PIPELINE_IMAGE_URL
        value: undefined
        type: text
      - name: CLUSTER_NAME
        value: ${NIFI_CLUSTER_NAME}
        type: text
    jobs:
      - name: Check dockerfile
        type: tester
        script: |-
          #!/bin/bash
          # uncomment to debug the script
          # set -x
          # copy the script below into your app code repo (e.g. ./scripts/check_dockerfile.sh) and 'source' it from your pipeline job
          #    source ./scripts/check_prebuild.sh
          # alternatively, you can source it from online script:
          #    source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_dockerfile.sh")
          # ------------------
          # source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_dockerfile.sh

          # This script lints Dockerfile.
          source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_dockerfile.sh")
        test_type: simple
      - name: Check registry
        type: builder
        artifact_dir: ""
        build_type: cr
        script: |-
          #!/bin/bash
          # uncomment to debug the script
          # set -x
          # copy the script below into your app code repo (e.g. ./scripts/check_registry.sh) and 'source' it from your pipeline job
          #    source ./scripts/check_registry.sh
          # alternatively, you can source it from online script:
          #    source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_registry.sh")
          # ------------------
          # source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_registry.sh

          # This script checks presence of registry namespace.
          source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_registry.sh")
        namespace: dev-openidl-aais-registry
        image_name: openidl-etl
        target:
          region_id: ${REGISTRY_REGION_ID}
          api_key: ${API_KEY}
          account_guid: 361c80aeecd44412b23a7f1bc4ae580f
      - name: Build container image
        type: builder
        artifact_dir: output
        build_type: cr
        script: |
          #!/bin/bash
          # uncomment to debug the script
          # set -x
          # copy the script below into your app code repo (e.g. ./scripts/build_image.sh) and 'source' it from your pipeline job
          #    source ./scripts/build_image.sh
          # alternatively, you can source it from online script:
          #    source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/build_image.sh")
          # ------------------
          # source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/build_image.sh

          # call custom defined script application specific <<
          echo ${DOCKER_ROOT}/kube-build.sh
          chmod 755 ${DOCKER_ROOT}/kube-build.sh
          . ${DOCKER_ROOT}/kube-build.sh
          # end of custom script  >>

          # This script does build a Docker image into IBM Container Service private image registry.
          # Minting image tag using format: BUILD_NUMBER-BRANCH-COMMIT_ID-TIMESTAMP
          # Also copies information into a build.properties file, so they can be reused later on by other scripts (e.g. image url, chart name, ...)
          source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/build_image.sh")
        namespace: dev-openidl-aais-registry
        image_name: openidl-etl
        target:
          region_id: ${REGISTRY_REGION_ID}
          api_key: ${API_KEY}
          account_guid: 361c80aeecd44412b23a7f1bc4ae580f
      - name: Check vulnerabilities
        type: tester
        fail_stage: false
        script: |
          #!/bin/bash
          # uncomment to debug the script
          # set -x
          # copy the script below into your app code repo (e.g. ./scripts/check_vulnerabilities.sh) and 'source' it from your pipeline job
          #    source ./scripts/check_vulnerabilities.sh
          # alternatively, you can source it from online script:
          #    source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_vulnerabilities.sh")
          # ------------------
          # source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_vulnerabilities.sh
          # Check for vulnerabilities of built image using Vulnerability Advisor
          source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_vulnerabilities.sh")
        namespace: default
        image_name: pipelineBuiltImage
        image_tag: latest
        target:
          region_id: ${REGISTRY_REGION_ID}
          api_key: ${API_KEY}
          account_guid: 361c80aeecd44412b23a7f1bc4ae580f
        test_type: vulnerabilityadvisor
  - name: DEPLOY
    inputs:
      - type: job
        stage: CONTAINERIZE
        job: Build container image
        dir_name: null
    triggers:
      - type: stage
    permission:
      execute: TOOLCHAIN_ADMINS
    properties:
      - name: buildprops
        value: build.properties
        type: file
      - name: CLUSTER_NAMESPACE
        value: nifi
        type: text
      - name: DEPLOYMENT_FILE
        value: nifi-cluster.yaml
        type: text
      - name: APP_URL
        value: undefined
        type: text
      - name: DEPLOYMENT_NAME
        value: etl-deployment
        type: text
    jobs:
      - name: Deploy to Kubernetes
        type: deployer
        deploy_type: kubernetes
        target:
          region_id: ${REGISTRY_REGION_ID}
          api_key: ${API_KEY}
          account_guid: 361c80aeecd44412b23a7f1bc4ae580f
          resource_group: ${PROD_RESOURCE_GROUP}
          kubernetes_cluster: ${PROD_NIFI_CLUSTER_NAME}
        script: "#!/bin/bash\n# This script checks the IBM Container Service cluster is ready, has a namespace configured with access to the private\n# image registry (using an IBM Cloud API Key), perform a kubectl deploy of container image and check on outcome.\n# uncomment to debug the script\n# set -x\n# copy the script below into your app code repo (e.g. ./scripts/check_and_deploy_kubectl.sh) and 'source' it from your pipeline job\n#    source ./scripts/check_and_deploy_kubectl.sh\n# alternatively, you can source it from online script:\n#    source <(curl -sSL \"https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_and_deploy_kubectl.sh\")\n# ------------------\n# source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_and_deploy_kubectl.sh\n\n# This script checks the IBM Container Service cluster is ready, has a namespace configured with access to the private\n# image registry (using an IBM Cloud API Key), perform a kubectl deploy of container image and check on outcome.\n\n# Input env variables (can be received via a pipeline environment properties.file.\necho \"IMAGE_NAME=${IMAGE_NAME}\"\necho \"IMAGE_TAG=${IMAGE_TAG}\"\necho \"REGISTRY_URL=${REGISTRY_URL}\"\necho \"REGISTRY_NAMESPACE=${REGISTRY_NAMESPACE}\"\necho \"DEPLOYMENT_FILE=${DEPLOYMENT_FILE}\"\necho \"USE_ISTIO_GATEWAY=${USE_ISTIO_GATEWAY}\"\necho \"KUBERNETES_SERVICE_ACCOUNT_NAME=${KUBERNETES_SERVICE_ACCOUNT_NAME}\"\n\necho \"Use for custom Kubernetes cluster target:\"\necho \"KUBERNETES_MASTER_ADDRESS=${KUBERNETES_MASTER_ADDRESS}\"\necho \"KUBERNETES_MASTER_PORT=${KUBERNETES_MASTER_PORT}\"\necho \"KUBERNETES_SERVICE_ACCOUNT_TOKEN=${KUBERNETES_SERVICE_ACCOUNT_TOKEN}\"\n\n# View build properties\nif [ -f build.properties ]; then \n  echo \"build.properties:\"\n  cat build.properties | grep -v -i password\nelse \n  echo \"build.properties : not found\"\nfi \n# also run 'env' command to find all available env variables\n# or learn more about the available environment variables at:\n# https://cloud.ibm.com/docs/services/ContinuousDelivery/pipeline_deploy_var.html#deliverypipeline_environment\n\n# Input env variables from pipeline job\necho \"PIPELINE_KUBERNETES_CLUSTER_NAME=${PIPELINE_KUBERNETES_CLUSTER_NAME}\"\necho \"CLUSTER_NAMESPACE=${CLUSTER_NAMESPACE}\"\n\n# If custom cluster credentials available, connect to this cluster instead\nif [ ! -z \"${KUBERNETES_MASTER_ADDRESS}\" ]; then\n  kubectl config set-cluster custom-cluster --server=https://${KUBERNETES_MASTER_ADDRESS}:${KUBERNETES_MASTER_PORT} --insecure-skip-tls-verify=true\n  kubectl config set-credentials sa-user --token=\"${KUBERNETES_SERVICE_ACCOUNT_TOKEN}\"\n  kubectl config set-context custom-context --cluster=custom-cluster --user=sa-user --namespace=\"${CLUSTER_NAMESPACE}\"\n  kubectl config use-context custom-context\nfi\n# Use kubectl auth to check if the kubectl client configuration is appropriate\n# check if the current configuration can create a deployment in the target namespace\necho \"Check ability to create a kubernetes deployment in ${CLUSTER_NAMESPACE} using kubectl CLI\"\nkubectl auth can-i create deployment --namespace ${CLUSTER_NAMESPACE}\n\n#Check cluster availability\necho \"==========================================================\"\necho \"CHECKING CLUSTER readiness and namespace existence\"\nif [ -z \"${KUBERNETES_MASTER_ADDRESS}\" ]; then\n  IP_ADDR=$( ibmcloud ks workers --cluster ${PIPELINE_KUBERNETES_CLUSTER_NAME} | grep normal | head -n 1 | awk '{ print $2 }' )\n  if [ -z \"${IP_ADDR}\" ]; then\n    echo -e \"${PIPELINE_KUBERNETES_CLUSTER_NAME} not created or workers not ready\"\n    exit 1\n  fi\nfi\necho \"Configuring cluster namespace\"\nif kubectl get namespace ${CLUSTER_NAMESPACE}; then\n  echo -e \"Namespace ${CLUSTER_NAMESPACE} found.\"\nelse\n  kubectl create namespace ${CLUSTER_NAMESPACE}\n  echo -e \"Namespace ${CLUSTER_NAMESPACE} created.\"\nfi\n\n# Grant access to private image registry from namespace $CLUSTER_NAMESPACE\n# reference https://cloud.ibm.com/docs/containers?topic=containers-images#other_registry_accounts\necho \"==========================================================\"\necho -e \"CONFIGURING ACCESS to private image registry from namespace ${CLUSTER_NAMESPACE}\"\nIMAGE_PULL_SECRET_NAME=\"ibmcloud-toolchain-${PIPELINE_TOOLCHAIN_ID}-${REGISTRY_URL}\"\n\necho -e \"Checking for presence of ${IMAGE_PULL_SECRET_NAME} imagePullSecret for this toolchain\"\nif ! kubectl get secret ${IMAGE_PULL_SECRET_NAME} --namespace ${CLUSTER_NAMESPACE}; then\n  echo -e \"${IMAGE_PULL_SECRET_NAME} not found in ${CLUSTER_NAMESPACE}, creating it\"\n  # for Container Registry, docker username is 'token' and email does not matter\n  if [ -z \"${PIPELINE_BLUEMIX_API_KEY}\" ]; then PIPELINE_BLUEMIX_API_KEY=${IBM_CLOUD_API_KEY}; fi #when used outside build-in kube job\n  kubectl --namespace ${CLUSTER_NAMESPACE} create secret docker-registry ${IMAGE_PULL_SECRET_NAME} --docker-server=${REGISTRY_URL} --docker-password=${PIPELINE_BLUEMIX_API_KEY} --docker-username=iamapikey --docker-email=a@b.com\nelse\n  echo -e \"Namespace ${CLUSTER_NAMESPACE} already has an imagePullSecret for this toolchain.\"\nfi\nif [ -z \"${KUBERNETES_SERVICE_ACCOUNT_NAME}\" ]; then KUBERNETES_SERVICE_ACCOUNT_NAME=\"default\" ; fi\nSERVICE_ACCOUNT=$(kubectl get serviceaccount ${KUBERNETES_SERVICE_ACCOUNT_NAME}  -o json --namespace ${CLUSTER_NAMESPACE} )\nif ! echo ${SERVICE_ACCOUNT} | jq -e '. | has(\"imagePullSecrets\")' > /dev/null ; then\n  kubectl patch --namespace ${CLUSTER_NAMESPACE} serviceaccount/${KUBERNETES_SERVICE_ACCOUNT_NAME} -p '{\"imagePullSecrets\":[{\"name\":\"'\"${IMAGE_PULL_SECRET_NAME}\"'\"}]}'\nelse\n  if echo ${SERVICE_ACCOUNT} | jq -e '.imagePullSecrets[] | select(.name==\"'\"${IMAGE_PULL_SECRET_NAME}\"'\")' > /dev/null ; then \n    echo -e \"Pull secret already found in ${KUBERNETES_SERVICE_ACCOUNT_NAME} serviceAccount\"\n  else\n    echo \"Inserting toolchain pull secret into ${KUBERNETES_SERVICE_ACCOUNT_NAME} serviceAccount\"\n    kubectl patch --namespace ${CLUSTER_NAMESPACE} serviceaccount/${KUBERNETES_SERVICE_ACCOUNT_NAME} --type='json' -p='[{\"op\":\"add\",\"path\":\"/imagePullSecrets/-\",\"value\":{\"name\": \"'\"${IMAGE_PULL_SECRET_NAME}\"'\"}}]'\n  fi\nfi\necho \"${KUBERNETES_SERVICE_ACCOUNT_NAME} serviceAccount:\"\nkubectl get serviceaccount ${KUBERNETES_SERVICE_ACCOUNT_NAME} --namespace ${CLUSTER_NAMESPACE} -o yaml\necho -e \"Namespace ${CLUSTER_NAMESPACE} authorizing with private image registry using patched ${KUBERNETES_SERVICE_ACCOUNT_NAME} serviceAccount\"\n\necho \"==========================================================\"\necho \"CHECKING DEPLOYMENT.YML manifest\"\nif [ -z \"${DEPLOYMENT_FILE}\" ]; then DEPLOYMENT_FILE=deployment.yml ; fi\nif [ ! -f ${DEPLOYMENT_FILE} ]; then\n  echo \"No ${DEPLOYMENT_FILE} found. Initializing it.\"\n  deployment_content=$(cat <<'EOT'\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: %s\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: %s\n  template:\n    metadata:\n      labels:\n        app: %s\n    spec:\n      containers:\n      - name: %s\n        image: %s\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: %s\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: %s\n  labels:\n    app: %s\nspec:\n  type: NodePort\n  ports:\n    - port: %s\n  selector:\n    app: %s\nEOT\n)\n  # Find the port\n  PORT=$(ibmcloud cr image-inspect \"${REGISTRY_URL}/${REGISTRY_NAMESPACE}/${IMAGE_NAME}:${IMAGE_TAG}\" --format '{{ range $key,$value := .ContainerConfig.ExposedPorts }}   ' | sed -E 's/^[^0-9]*([0-9]+).*$/\\1/') || true\n  if [ \"$PORT\" -eq \"$PORT\" ] 2>/dev/null; then\n    echo \"ExposedPort $PORT found while inspecting image ${REGISTRY_URL}/${REGISTRY_NAMESPACE}/${IMAGE_NAME}:${IMAGE_TAG}\"\n  else \n    echo \"Found '$PORT' as ExposedPort while inspecting image ${REGISTRY_URL}/${REGISTRY_NAMESPACE}/${IMAGE_NAME}:${IMAGE_TAG}, non numeric value so using 5000 as containerPort\"\n    PORT=5000\n  fi\n  # Generate deployment file  \n  echo \"GENERATED ${DEPLOYMENT_FILE}:\"\n  # Derive an application name from toolchain name ensuring it is conform to DNS-1123 subdomain\n  application_name=$(echo ${IDS_PROJECT_NAME} | tr -cd '[:alnum:].-')\n  printf \"$deployment_content\" \\\n   \"${application_name}\" \"${application_name}\" \"${application_name}\" \"${application_name}\" \"${REGISTRY_URL}/${REGISTRY_NAMESPACE}/${IMAGE_NAME}:${IMAGE_TAG}\" \"${PORT}\" \\\n   \"${application_name}\" \"${application_name}\" \"${PORT}\" \"${application_name}\" | tee ${DEPLOYMENT_FILE}\nfi\n\necho \"==========================================================\"\necho \"UPDATING manifest with image information\"\nIMAGE_REPOSITORY=${REGISTRY_URL}/${REGISTRY_NAMESPACE}/${IMAGE_NAME}\necho -e \"Updating ${DEPLOYMENT_FILE} with image name: ${IMAGE_REPOSITORY}:${IMAGE_TAG}\"\nNEW_DEPLOYMENT_FILE=\"$(dirname $DEPLOYMENT_FILE)/tmp.$(basename $DEPLOYMENT_FILE)\"\n# find the yaml document index for the K8S deployment definition\nDEPLOYMENT_DOC_INDEX=$(yq read --doc \"*\" --tojson $DEPLOYMENT_FILE | jq -r 'to_entries | .[] | select(.value.kind | ascii_downcase==\"deployment\") | .key')\nif [ -z \"$DEPLOYMENT_DOC_INDEX\" ]; then\n  echo \"No Kubernetes Deployment definition found in $DEPLOYMENT_FILE. Updating YAML document with index 0\"\n  DEPLOYMENT_DOC_INDEX=0\nfi\n# Update deployment with image name\nyq write $DEPLOYMENT_FILE --doc $DEPLOYMENT_DOC_INDEX \"spec.template.spec.containers[0].image\" \"${REGISTRY_URL}/${REGISTRY_NAMESPACE}/${IMAGE_NAME}:${IMAGE_TAG}\" > ${NEW_DEPLOYMENT_FILE}\nDEPLOYMENT_FILE=${NEW_DEPLOYMENT_FILE} # use modified file\ncat ${DEPLOYMENT_FILE}\n\necho \"==========================================================\"\necho \"DEPLOYING using manifest\"\nset -x\nkubectl apply --namespace ${CLUSTER_NAMESPACE} -f ${DEPLOYMENT_FILE} \nset +x\n# Extract name from actual Kube deployment resource owning the deployed container image \n# Ensure that the image match the repository, image name and tag without the @ sha id part to handle\n# case when image is sha-suffixed or not - ie:\n# us.icr.io/sample/hello-containers-20190823092122682:1-master-a15bd262-20190823100927\n# or\n# us.icr.io/sample/hello-containers-20190823092122682:1-master-a15bd262-20190823100927@sha256:9b56a4cee384fa0e9939eee5c6c0d9912e52d63f44fa74d1f93f3496db773b2e\nDEPLOYMENT_NAME=$(kubectl get statefulset --namespace ${CLUSTER_NAMESPACE} -o json | jq -r '.items[] | select(.spec.template.spec.containers[]?.image | test(\"'\"${IMAGE_REPOSITORY}:${IMAGE_TAG}\"'(@.+|$)\")) | .metadata.name' )\necho -e \"CHECKING deployment rollout of ${DEPLOYMENT_NAME}\"\necho \"\"\nset -x\nif kubectl rollout status statefulset/${DEPLOYMENT_NAME} --watch=true --timeout=${ROLLOUT_TIMEOUT:-\"600s\"} --namespace ${CLUSTER_NAMESPACE}; then\n  STATUS=\"pass\"\nelse\n  STATUS=\"fail\"\nfi\nset +x\n\n# Dump events that occured during the rollout\necho \"SHOWING last events\"\nkubectl get events --sort-by=.metadata.creationTimestamp -n ${CLUSTER_NAMESPACE}\n\n# Record deploy information\nif jq -e '.services[] | select(.service_id==\"draservicebroker\")' _toolchain.json > /dev/null 2>&1; then\n  if [ -z \"${KUBERNETES_MASTER_ADDRESS}\" ]; then\n    DEPLOYMENT_ENVIRONMENT=\"${PIPELINE_KUBERNETES_CLUSTER_NAME}:${CLUSTER_NAMESPACE}\"\n  else \n    DEPLOYMENT_ENVIRONMENT=\"${KUBERNETES_MASTER_ADDRESS}:${CLUSTER_NAMESPACE}\"\n  fi\n  ibmcloud doi publishdeployrecord --env $DEPLOYMENT_ENVIRONMENT \\\n    --buildnumber ${SOURCE_BUILD_NUMBER} --logicalappname=\"${APP_NAME:-$IMAGE_NAME}\" --status ${STATUS}\nfi\nif [ \"$STATUS\" == \"fail\" ]; then\n  echo \"DEPLOYMENT FAILED\"\n  echo \"Showing registry pull quota\"\n  ibmcloud cr quota || true\n  exit 1\nfi\n# Extract app name from actual Kube pod \n# Ensure that the image match the repository, image name and tag without the @ sha id part to handle\n# case when image is sha-suffixed or not - ie:\n# us.icr.io/sample/hello-containers-20190823092122682:1-master-a15bd262-20190823100927\n# or\n# us.icr.io/sample/hello-containers-20190823092122682:1-master-a15bd262-20190823100927@sha256:9b56a4cee384fa0e9939eee5c6c0d9912e52d63f44fa74d1f93f3496db773b2e\necho \"==========================================================\"\nAPP_NAME=$(kubectl get pods --namespace ${CLUSTER_NAMESPACE} -o json | jq -r '[ .items[] | select(.spec.containers[]?.image | test(\"'\"${IMAGE_REPOSITORY}:${IMAGE_TAG}\"'(@.+|$)\")) | .metadata.labels.app] [0]')\necho -e \"APP: ${APP_NAME}\"\necho \"DEPLOYED PODS:\"\nkubectl describe pods --selector app=${APP_NAME} --namespace ${CLUSTER_NAMESPACE}\n\n# lookup service for current release\nAPP_SERVICE=$(kubectl get services --namespace ${CLUSTER_NAMESPACE} -o json | jq -r ' .items[] | select (.spec.selector.release==\"'\"${RELEASE_NAME}\"'\") | .metadata.name ')\nif [ -z \"${APP_SERVICE}\" ]; then\n  # lookup service for current app\n  APP_SERVICE=$(kubectl get services --namespace ${CLUSTER_NAMESPACE} -o json | jq -r ' .items[] | select (.spec.selector.app==\"'\"${APP_NAME}\"'\") | .metadata.name ')\nfi\nif [ ! -z \"${APP_SERVICE}\" ]; then\n  echo -e \"SERVICE: ${APP_SERVICE}\"\n  echo \"DEPLOYED SERVICES:\"\n  kubectl describe services ${APP_SERVICE} --namespace ${CLUSTER_NAMESPACE}\nfi\n\necho \"\"\necho \"==========================================================\"\necho \"DEPLOYMENT SUCCEEDED\"\nif [ ! -z \"${APP_SERVICE}\" ]; then\n  echo \"\"\n  if [ \"${USE_ISTIO_GATEWAY}\" = true ]; then\n    PORT=$( kubectl get svc istio-ingressgateway -n istio-system -o json | jq -r '.spec.ports[] | select (.name==\"http2\") | .nodePort ' )\n    echo -e \"*** istio gateway enabled ***\"\n  else\n    PORT=$( kubectl get services --namespace ${CLUSTER_NAMESPACE} | grep ${APP_SERVICE} | sed 's/.*:\\([0-9]*\\).*/\\1/g' )\n  fi\n  if [ -z \"${KUBERNETES_MASTER_ADDRESS}\" ]; then\n    echo \"Using first worker node ip address as NodeIP: ${IP_ADDR}\"\n  else \n    # check if a route resource exists in the this kubernetes cluster\n    if kubectl explain route > /dev/null 2>&1; then\n      # Assuming the kubernetes target cluster is an openshift cluster\n      # Check if a route exists for exposing the service ${APP_SERVICE}\n      if  kubectl get routes --namespace ${CLUSTER_NAMESPACE} -o json | jq --arg service \"$APP_SERVICE\" -e '.items[] | select(.spec.to.name==$service)'; then\n        echo \"Existing route to expose service $APP_SERVICE\"\n      else\n        # create OpenShift route\ncat > test-route.json << EOF\n{\"apiVersion\":\"route.openshift.io/v1\",\"kind\":\"Route\",\"metadata\":{\"name\":\"${APP_SERVICE}\"},\"spec\":{\"to\":{\"kind\":\"Service\",\"name\":\"${APP_SERVICE}\"}}}\nEOF\n        echo \"\"\n        cat test-route.json\n        kubectl apply -f test-route.json --validate=false --namespace ${CLUSTER_NAMESPACE}\n        kubectl get routes --namespace ${CLUSTER_NAMESPACE}\n      fi\n      echo \"LOOKING for host in route exposing service $APP_SERVICE\"\n      IP_ADDR=$(kubectl get routes --namespace ${CLUSTER_NAMESPACE} -o json | jq --arg service \"$APP_SERVICE\" -r '.items[] | select(.spec.to.name==$service) | .status.ingress[0].host')\n      PORT=80\n    else\n      # Use the KUBERNETES_MASTER_ADRESS\n      IP_ADDR=${KUBERNETES_MASTER_ADDRESS}\n    fi\n  fi  \n  export APP_URL=http://${IP_ADDR}:${PORT} # using 'export', the env var gets passed to next job in stage\n  echo -e \"VIEW THE APPLICATION AT: ${APP_URL}\"\nfi"
      - name: Check health
        type: deployer
        deploy_type: kubernetes
        target:
          region_id: ${REGISTRY_REGION_ID}
          api_key: ${API_KEY}
          account_guid: 361c80aeecd44412b23a7f1bc4ae580f
          resource_group: ${PROD_RESOURCE_GROUP}
          kubernetes_cluster: dev-openidl-aais-nifi-cluster
        script: |-
          #!/bin/bash
          # uncomment to debug the script
          # set -x
          # copy the script below into your app code repo (e.g. ./scripts/check_health.sh) and 'source' it from your pipeline job
          #    source ./scripts/check_health.sh
          # alternatively, you can source it from online script:
          #    source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_health.sh")
          # ------------------
          # source: https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_health.sh
          # Check liveness and readiness probes to confirm application is healthy
          source <(curl -sSL "https://raw.githubusercontent.com/open-toolchain/commons/master/scripts/check_health.sh")
properties:
  - name: IBM_CLOUD_API_KEY
    type: secure
